// api/ai-advanced.js - Sistema de IA Avan√ßada para o VeloBot
const OpenAI = require('openai');
const axios = require('axios');

// Configura√ß√£o do OpenAI
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Cache para embeddings
const embeddingsCache = new Map();
const CACHE_DURATION = 24 * 60 * 60 * 1000; // 24 horas

// ==================== 1. AN√ÅLISE SEM√ÇNTICA COM EMBEDDINGS ====================

async function getEmbedding(text) {
  const cacheKey = text.toLowerCase().trim();
  const cached = embeddingsCache.get(cacheKey);
  
  if (cached && Date.now() - cached.timestamp < CACHE_DURATION) {
    return cached.embedding;
  }

  try {
    const response = await openai.embeddings.create({
      model: "text-embedding-3-small",
      input: text,
    });
    
    const embedding = response.data[0].embedding;
    embeddingsCache.set(cacheKey, {
      embedding,
      timestamp: Date.now()
    });
    
    return embedding;
  } catch (error) {
    console.error('Erro ao gerar embedding:', error);
    return null;
  }
}

function cosineSimilarity(a, b) {
  if (!a || !b || a.length !== b.length) return 0;
  
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;
  
  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }
  
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

async function buscaSemantica(pergunta, faqData) {
  const perguntaEmbedding = await getEmbedding(pergunta);
  if (!perguntaEmbedding) return [];

  const cabecalho = faqData[0];
  const dados = faqData.slice(1);
  const idxPergunta = cabecalho.indexOf("Pergunta");
  const idxResposta = cabecalho.indexOf("Resposta");
  const idxPalavrasChave = cabecalho.indexOf("Palavras-chave");

  if (idxPergunta === -1 || idxResposta === -1) return [];

  const resultados = [];

  for (let i = 0; i < dados.length; i++) {
    const linha = dados[i];
    const perguntaItem = linha[idxPergunta];
    const respostaItem = linha[idxResposta];
    const palavrasChave = linha[idxPalavrasChave];

    if (!perguntaItem || !respostaItem) continue;

    // Criar texto combinado para embedding
    const textoCombinado = `${perguntaItem} ${palavrasChave || ''}`;
    const itemEmbedding = await getEmbedding(textoCombinado);
    
    if (itemEmbedding) {
      const similaridade = cosineSimilarity(perguntaEmbedding, itemEmbedding);
      
      if (similaridade > 0.7) { // Threshold de similaridade
        resultados.push({
          pergunta: perguntaItem,
          resposta: respostaItem,
          similaridade,
          sourceRow: i + 2,
          tipo: 'semantica'
        });
      }
    }
  }

  return resultados.sort((a, b) => b.similaridade - a.similaridade);
}

// ==================== 2. CLASSIFICA√á√ÉO DE INTEN√á√ÉO ====================

async function classificarIntencao(pergunta, historico = []) {
  const prompt = `
Analise a inten√ß√£o desta pergunta e classifique em UMA das categorias:

CATEGORIAS:
- CONSULTA: Quer saber informa√ß√µes gerais
- PROBLEMA: Relatando um erro, bug ou dificuldade t√©cnica
- PROCEDIMENTO: Como fazer algo espec√≠fico (passo a passo)
- STATUS: Verificar situa√ß√£o, andamento, status de algo
- URGENTE: Problema cr√≠tico que precisa resolu√ß√£o imediata
- ESCLARECIMENTO: D√∫vida sobre algo j√° mencionado
- OUTRO: N√£o se encaixa nas anteriores

CONTEXTO DA CONVERSA:
${historico.map(h => `${h.role}: ${h.content}`).join('\n')}

PERGUNTA: "${pergunta}"

Responda APENAS com a categoria (ex: CONSULTA) e uma breve justificativa (ex: Usu√°rio quer saber informa√ß√µes sobre cr√©dito):
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.1,
      max_tokens: 100
    });

    const resultado = response.choices[0].message.content.trim();
    const [categoria, justificativa] = resultado.split('\n');
    
    return {
      categoria: categoria.replace(/[^A-Z_]/g, ''),
      justificativa: justificativa || categoria,
      confianca: 0.9
    };
  } catch (error) {
    console.error('Erro na classifica√ß√£o de inten√ß√£o:', error);
    return { categoria: 'OUTRO', justificativa: 'Erro na classifica√ß√£o', confianca: 0.1 };
  }
}

// ==================== 3. AN√ÅLISE DE SENTIMENTO E URG√äNCIA ====================

async function analisarUrgenciaESentimento(pergunta) {
  const prompt = `
Analise esta pergunta e responda em JSON:

PERGUNTA: "${pergunta}"

Responda com:
{
  "urgencia": 1-5,
  "sentimento": "POSITIVO|NEUTRO|NEGATIVO|FRUSTRADO",
  "palavras_chave_emocionais": ["palavra1", "palavra2"],
  "justificativa": "breve explica√ß√£o"
}

URG√äNCIA:
1 = Consulta geral
2 = D√∫vida sobre procedimento
3 = Problema que precisa resolver
4 = Erro cr√≠tico
5 = Emerg√™ncia

SENTIMENTO:
- POSITIVO: Pergunta educada, agradecimento
- NEUTRO: Pergunta normal, sem emo√ß√£o
- NEGATIVO: Reclama√ß√£o, insatisfa√ß√£o
- FRUSTRADO: M√∫ltiplas tentativas, impaci√™ncia
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.1,
      max_tokens: 200
    });

    const resultado = JSON.parse(response.choices[0].message.content);
    return resultado;
  } catch (error) {
    console.error('Erro na an√°lise de urg√™ncia:', error);
    return {
      urgencia: 2,
      sentimento: 'NEUTRO',
      palavras_chave_emocionais: [],
      justificativa: 'Erro na an√°lise'
    };
  }
}

// ==================== 4. BUSCA H√çBRIDA INTELIGENTE ====================

async function buscaHibrida(pergunta, faqData, historico = []) {
  console.log('üîç Iniciando busca h√≠brida para:', pergunta);

  // 1. Busca sem√¢ntica
  const resultadosSemanticos = await buscaSemantica(pergunta, faqData);
  console.log(`üìä Resultados sem√¢nticos: ${resultadosSemanticos.length}`);

  // 2. Busca por palavras-chave (m√©todo atual)
  const resultadosKeywords = await buscaPorPalavrasChave(pergunta, faqData);
  console.log(`üìä Resultados keywords: ${resultadosKeywords.length}`);

  // 3. IA combina e ranqueia resultados
  const resultadosCombinados = await combinarERanquearResultados(
    pergunta, 
    resultadosSemanticos, 
    resultadosKeywords, 
    historico
  );

  return resultadosCombinados;
}

async function buscaPorPalavrasChave(pergunta, faqData) {
  const cabecalho = faqData[0];
  const dados = faqData.slice(1);
  const idxPergunta = cabecalho.indexOf("Pergunta");
  const idxPalavrasChave = cabecalho.indexOf("Palavras-chave");
  const idxResposta = cabecalho.indexOf("Resposta");

  if (idxPergunta === -1 || idxResposta === -1 || idxPalavrasChave === -1) {
    return [];
  }

  const palavrasDaBusca = pergunta.toLowerCase().split(' ').filter(p => p.length > 2);
  const resultados = [];

  for (let i = 0; i < dados.length; i++) {
    const linha = dados[i];
    const textoPalavrasChave = (linha[idxPalavrasChave] || '').toLowerCase();
    let score = 0;

    palavrasDaBusca.forEach(palavra => {
      if (textoPalavrasChave.includes(palavra)) score++;
    });

    if (score > 0) {
      resultados.push({
        pergunta: linha[idxPergunta],
        resposta: linha[idxResposta],
        score: score / palavrasDaBusca.length,
        sourceRow: i + 2,
        tipo: 'keywords'
      });
    }
  }

  return resultados.sort((a, b) => b.score - a.score);
}

async function combinarERanquearResultados(pergunta, semanticos, keywords, historico) {
  const prompt = `
Analise os resultados de busca e ranqueie-os por relev√¢ncia para a pergunta:

PERGUNTA: "${pergunta}"

HIST√ìRICO DA CONVERSA:
${historico.map(h => `${h.role}: ${h.content}`).join('\n')}

RESULTADOS SEM√ÇNTICOS:
${semanticos.map((r, i) => `${i+1}. ${r.pergunta} (similaridade: ${r.similaridade.toFixed(3)})`).join('\n')}

RESULTADOS POR PALAVRAS-CHAVE:
${keywords.map((r, i) => `${i+1}. ${r.pergunta} (score: ${r.score.toFixed(3)})`).join('\n')}

Responda em JSON com os 5 melhores resultados ranqueados:
{
  "resultados": [
    {
      "pergunta": "texto da pergunta",
      "resposta": "texto da resposta", 
      "relevancia": 0.0-1.0,
      "tipo": "semantica|keywords",
      "sourceRow": 123,
      "justificativa": "por que √© relevante"
    }
  ],
  "confianca_geral": 0.0-1.0,
  "recomendacao": "RESPOSTA_DIRETA|PRECISA_ESCLARECIMENTO|MULTIPLAS_OPCOES"
}
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.1,
      max_tokens: 1000
    });

    const resultado = JSON.parse(response.choices[0].message.content);
    return resultado;
  } catch (error) {
    console.error('Erro ao combinar resultados:', error);
    // Fallback: combinar resultados manualmente
    return combinarResultadosManual(semanticos, keywords);
  }
}

function combinarResultadosManual(semanticos, keywords) {
  const todos = [...semanticos, ...keywords];
  const unicos = new Map();

  todos.forEach(item => {
    const key = item.pergunta;
    if (!unicos.has(key) || item.relevancia > unicos.get(key).relevancia) {
      unicos.set(key, item);
    }
  });

  return {
    resultados: Array.from(unicos.values()).slice(0, 5),
    confianca_geral: 0.7,
    recomendacao: 'RESPOSTA_DIRETA'
  };
}

// ==================== 5. GERA√á√ÉO DE RESPOSTAS CONTEXTUAIS ====================

async function gerarRespostaContextual(pergunta, contexto, historico, intencao, urgencia) {
  const prompt = `
### PERSONA
Voc√™ √© o VeloBot, assistente oficial da Velotax. Responda de forma inteligente e contextual.

### AN√ÅLISE DA SITUA√á√ÉO
- Inten√ß√£o: ${intencao.categoria} (${intencao.justificativa})
- Urg√™ncia: ${urgencia.urgencia}/5
- Sentimento: ${urgencia.sentimento}

### HIST√ìRICO DA CONVERSA
${historico.map(h => `${h.role}: ${h.content}`).join('\n')}

### BASE DE CONHECIMENTO
${contexto}

### PERGUNTA ATUAL
"${pergunta}"

### INSTRU√á√ïES
- Analise o contexto da conversa para entender o que o usu√°rio realmente quer
- Use informa√ß√µes da base de conhecimento de forma natural e fluida
- Adapte o tom baseado na urg√™ncia e sentimento
- Se a pergunta for amb√≠gua, pe√ßa esclarecimento espec√≠fico
- Mantenha o tom profissional da Velotax
- Se for urgente (4-5), seja direto e ofere√ßa solu√ß√µes r√°pidas
- Se for frustrado, seja emp√°tico e ofere√ßa ajuda extra

### RESPOSTA:
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.3,
      max_tokens: 800
    });

    return response.choices[0].message.content;
  } catch (error) {
    console.error('Erro ao gerar resposta contextual:', error);
    return "Desculpe, n√£o consegui processar sua pergunta no momento. Tente novamente.";
  }
}

// ==================== 6. SISTEMA DE FOLLOW-UP INTELIGENTE ====================

async function gerarFollowUps(pergunta, resposta, contexto) {
  const prompt = `
Baseado na pergunta e resposta, sugira 3 perguntas de follow-up √∫teis:

PERGUNTA: "${pergunta}"
RESPOSTA: "${resposta}"
CONTEXTO: ${contexto}

Gere 3 perguntas de follow-up que o usu√°rio provavelmente teria:
1. Uma pergunta para mais detalhes
2. Uma pergunta relacionada
3. Uma pergunta sobre pr√≥ximos passos

Responda em JSON:
{
  "followups": [
    "Pergunta 1",
    "Pergunta 2", 
    "Pergunta 3"
  ],
  "sugestoes_relacionadas": [
    "T√≥pico relacionado 1",
    "T√≥pico relacionado 2"
  ]
}
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.4,
      max_tokens: 300
    });

    return JSON.parse(response.choices[0].message.content);
  } catch (error) {
    console.error('Erro ao gerar follow-ups:', error);
    return {
      followups: [],
      sugestoes_relacionadas: []
    };
  }
}

// ==================== 7. SUGEST√ïES PROATIVAS ====================

async function gerarSugestoesProativas(pergunta, historico, contexto) {
  const prompt = `
Analise a conversa e sugira informa√ß√µes proativas que podem ser √∫teis:

PERGUNTA ATUAL: "${pergunta}"
HIST√ìRICO: ${historico.map(h => h.content).join(' | ')}
CONTEXTO: ${contexto}

Sugira 2-3 informa√ß√µes proativas que podem ser relevantes:
- Informa√ß√µes complementares
- Procedimentos relacionados
- Avisos importantes
- Links √∫teis

Responda em JSON:
{
  "sugestoes_proativas": [
    {
      "titulo": "T√≠tulo da sugest√£o",
      "conteudo": "Conte√∫do da sugest√£o",
      "tipo": "INFO|AVISO|LINK|PROCEDIMENTO"
    }
  ]
}
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.3,
      max_tokens: 400
    });

    return JSON.parse(response.choices[0].message.content);
  } catch (error) {
    console.error('Erro ao gerar sugest√µes proativas:', error);
    return { sugestoes_proativas: [] };
  }
}

// ==================== 8. BUSCA EM DOCUMENTOS EXTERNOS ====================

async function buscarEmDocumentosExternos(pergunta) {
  const sites = [
    "https://www.gov.br/receitafederal",
    "https://cav.receita.fazenda.gov.br",
    "https://www.gov.br",
    "https://velotax.com.br"
  ];

  let contexto = "";
  const perguntaEmbedding = await getEmbedding(pergunta);

  for (const site of sites) {
    try {
      const { data } = await axios.get(site, { timeout: 5000 });
      const siteEmbedding = await getEmbedding(data.substring(0, 2000)); // Primeiros 2000 chars
      
      if (perguntaEmbedding && siteEmbedding) {
        const similaridade = cosineSimilarity(perguntaEmbedding, siteEmbedding);
        if (similaridade > 0.3) {
          contexto += `Fonte: ${site}\nRelev√¢ncia: ${similaridade.toFixed(3)}\n\n`;
        }
      }
    } catch (error) {
      console.error(`Erro ao processar ${site}:`, error.message);
    }
  }

  return contexto;
}

// ==================== 9. SISTEMA DE CONVERSA√á√ÉO NATURAL ====================

async function manterContextoConversacional(pergunta, historico) {
  const prompt = `
Analise se esta pergunta faz refer√™ncia ao contexto anterior:

HIST√ìRICO:
${historico.map(h => `${h.role}: ${h.content}`).join('\n')}

PERGUNTA ATUAL: "${pergunta}"

Identifique:
1. Se h√° refer√™ncias como "isso", "aquilo", "o que voc√™ disse"
2. Se √© continua√ß√£o de um t√≥pico anterior
3. Se precisa de contexto para ser entendida

Responda em JSON:
{
  "tem_referencia": true/false,
  "referencias": ["refer√™ncia 1", "refer√™ncia 2"],
  "pergunta_expandida": "pergunta com contexto expl√≠cito",
  "contexto_necessario": "contexto que precisa ser mantido"
}
`;

  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.1,
      max_tokens: 300
    });

    return JSON.parse(response.choices[0].message.content);
  } catch (error) {
    console.error('Erro ao analisar contexto:', error);
    return {
      tem_referencia: false,
      referencias: [],
      pergunta_expandida: pergunta,
      contexto_necessario: ""
    };
  }
}

// ==================== 10. FUN√á√ÉO PRINCIPAL DE IA AVAN√áADA ====================

async function processarComIA(pergunta, faqData, historico = [], email = null) {
  console.log('ü§ñ Iniciando processamento com IA avan√ßada...');

  try {
    // 1. An√°lise inicial
    const intencao = await classificarIntencao(pergunta, historico);
    const urgencia = await analisarUrgenciaESentimento(pergunta);
    const contexto = await manterContextoConversacional(pergunta, historico);

    console.log('üìä An√°lise:', { intencao: intencao.categoria, urgencia: urgencia.urgencia });

    // 2. Busca h√≠brida
    const buscaResultados = await buscaHibrida(pergunta, faqData, historico);
    
    // 3. Busca em documentos externos se necess√°rio
    let contextoExterno = "";
    if (buscaResultados.confianca_geral < 0.7) {
      contextoExterno = await buscarEmDocumentosExternos(pergunta);
    }

    // 4. Gerar resposta contextual
    const contextoCompleto = `
${buscaResultados.resultados.map(r => `P: ${r.pergunta}\nR: ${r.resposta}`).join('\n\n')}

${contextoExterno}
    `.trim();

    const resposta = await gerarRespostaContextual(
      pergunta, 
      contextoCompleto, 
      historico, 
      intencao, 
      urgencia
    );

    // 5. Gerar follow-ups e sugest√µes
    const followUps = await gerarFollowUps(pergunta, resposta, contextoCompleto);
    const sugestoesProativas = await gerarSugestoesProativas(pergunta, historico, contextoCompleto);

    // 6. Preparar resposta final
    const respostaFinal = {
      status: "sucesso_ia_avancada",
      resposta,
      intencao: intencao.categoria,
      urgencia: urgencia.urgencia,
      sentimento: urgencia.sentimento,
      confianca: buscaResultados.confianca_geral,
      recomendacao: buscaResultados.recomendacao,
      followups: followUps.followups,
      sugestoes_relacionadas: followUps.sugestoes_relacionadas,
      sugestoes_proativas: sugestoesProativas.sugestoes_proativas,
      contexto_usado: buscaResultados.resultados.length,
      source: "IA Avan√ßada"
    };

    console.log('‚úÖ Processamento conclu√≠do:', {
      intencao: intencao.categoria,
      urgencia: urgencia.urgencia,
      confianca: buscaResultados.confianca_geral
    });

    return respostaFinal;

  } catch (error) {
    console.error('‚ùå Erro no processamento IA:', error);
    return {
      status: "erro_ia",
      resposta: "Desculpe, ocorreu um erro no processamento. Tente novamente.",
      source: "Sistema"
    };
  }
}

module.exports = {
  processarComIA,
  classificarIntencao,
  analisarUrgenciaESentimento,
  buscaHibrida,
  gerarFollowUps,
  gerarSugestoesProativas,
  manterContextoConversacional,
  buscarEmDocumentosExternos
};
